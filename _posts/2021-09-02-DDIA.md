---
layout: post
title: "What I learned from Designing Data-Intensive Applications (the book) - Part 1"
---

I recently picked up a copy of the book *Designing Data-Intensive Applications* by Martin Kleppmann because I'm not entirely comfortable using technologies of which I don't quite understand the nuts and bolts. It's really easy to use some of the tools available today and have no idea what the inner working look like, and for many people that's fine. You don't always need to be the expert in a technology, because there are *tons* of resources online and hopefully, there is someone at your organization who *is* an expert in that technology. I guess that's the real reason I bought the book - I want to be the person with the answers. This book has outstanding reviews and discuesses the techniques used in data engineering that we use all the time on an intimate level, the level behind all of the abstractions. I'm extremely excited to learn about the nitty gritty details and I’ve found that writing about a topic is a great way to reveal what you don’t *really* know. So, in that vein, I’m going to discuss the cool things I’ve learned to try to solidify my understanding.

Reliability, scalability, maintainability: These are the pillars of an effective, efficient data architecture – what we strive for when designing data-intensive applications. The ideas are simple: I know what the words mean and why the concepts are important. But I’ve found this book is very helpful in nailing down the details of what reliable, scalable, and maintainable design. Following the order of the chapter, I’ll start with reliability
## Reliability
A reliable system is a system that continues to perform its expected function (so, working correctly) in, well, let’s say *undesirable circumstances.* These undesirable circumstances could come in the form of software errors, unexpected user input, or just plain old hardware failures – in short, faults. We want to adequately deal with faults, so that faults don’t cause failure, that is, the system ceasing to operate at all. So, the question becomes how does one build a reliable system? What are the concrete steps we can take to make sure that faults don’t lead to failures? 
##### Hardware
I was a bit surprised to learn that, while it’s common to expect large compute clusters to average one dead hard disk per day, hardware faults are a significant factor in only 10-25% of system failures. That’s interesting! It seems that well thought out and effective solutions are already in place and are common practice in the industry. As a data engineer/architect in a software company, one may not actually have to worry all that much about the hardware faults. Only so many companies will have huge on-site clusters. I believe it’s reasonable to assume that reliability measures such as power supply fail-safes will be discussed in the service level agreements (SLAs) of large data houses if required. Something we may want to consider however are how we store data on the physical devices. I believe the simplest and most common technique to ensure data availability on hard disk is to add redundancy to the system – an example being storing the same data on multiple disks. In the event of a disk failure (remember that this can be expected to happen every day), data can still be accessed from the redundant drive while the faulty disk is repaired, or a backup is rebooted.
##### Software
Reliable software comes down to testing. A lot of testing. Thorough testing and seeking out edge cases that *shouldn’t* ever come up, but at some point before the heat death of the universe *will probably come up.* I won’t go into detail listing a bunch of arbitrary types of bugs, but generally, we’ll want to implement measures such as test-driven-design/unit testing, monitoring performance of processes, and isolating the processes from one another to test their performance.

## Scalability – load and performance
*But can it scale????* That’s a bad question, but one that is asked a lot. I’ve also been guilty of thinking about scalability without having any real concerns in mind. Kleppmann and others describe scalability as a system’s ability to cope with increased load. So before we even talk about how well an application scales, we have to learn how to discuss load and performance, what increased load might look like, and how to monitor load and performance. Concrete questions you can use as a starting point are “**If the system grows in a particular way, what are our options for coping with the growth?**” and “**How can we add computing resources to handle the additional load?**”
So, what are we talking about when talk about load? That depends on the application. If we were to run a blogging platform like tumblr, load might be refresh rate or the number of blogs being written and posted (database writes). For a video streaming platform like twitch, load could be the number of active chatters/viewers in a twitch channel or the number of people streaming simultaneously. These load considerations are called *load parameters.* Kleppmann uses Twitter as a case study to discuss load, for which the two most frequent operations, and hence the source of most of the load experienced by Twitter, are *post tweet* and *fetch home timeline.* The load associated with these two operations are reads and writes to a database. Metrics associated with posting tweets: average of 4.6k requests/second, peak of 12k requests/second. Metrics associated with fetching the home timeline: 300k requests/second. Twitter’s scaling challenge is not handling 12k database writes/second; the challenge arises from *fan-out.* An example of fan-out can go something like this: User 1 posts a tweet -> this tweet must be added to the home timeline of all of his or her (potentially) millions of followers. In turn, User 1 also follows many other people, and all of their tweets must also be viewed in User 1’s home timeline. There are probably more than a billion Twitter users, right? Hundreds of millions of users log in and fetch their home timeline probably multiple times per session - all of this is happening concurrently, remember. This ends up being *a ton* of database reads and writes, happening all the time. Kleppmann discusses two approaches to handling this enormous amount of load. The initial approach Twitter used to fetch a timeline, was essentially **an SQL JOIN statement**. When a user requests their timeline, look up who they follow, find their tweets, merge, sort, then display it for the user. Easy peasy. However, Twitter found that this approach struggled to keep up with the 300k requests/second. This necessitated a switch to the second approach – **maintaining a cache for each user’s home timeline.**  This is clever approach! The average rate of published tweets is almost two orders of magnitude less than the rate of home timeline reads – it makes a lot of sense to do the heavy lifting during database writes, which makes the frequent read requests very cheap! *This is very cool!* The two approaches have fundamentally different ways of serving the same information. Approach 1 computes its info during read time, approach 2 computes everything at write time. I loved this discussion because it made me realize the importance of looking at load from various angles.
I talked a lot about load and not a lot about performance so far. Performance seems a little easier to intuitively understand the details, but there are some important details that I took away from the discussion of performance. The major concept is that of *tail latencies.* Before reading this book, it’s likely that I wouldn’t have put too much thought into what the response time for the 99th percentile looks like, and definitely not the response time of the 99.9th percentile. But understanding the Amazon perspective of these metrics shows just how important these response times are – “Amazon describes response time requirements for internal services in terms of the 99.9th percentile, even though it only affects 1 in 1,000 requests. This is because the customers with the slowest requests are often those who have the most data on their accounts because they have made many purchases—that is, they’re the most valuable customers” That’s pretty eye-opening right? **It’s the 0.1% of response times that make the most difference in their business**. Some of the easiest to ignore response times are the most crucial! 
What might cause high tail latencies? It might be queuing delays. It only takes one or two slow requests to hold up the processing of all subsequent requests. These subsequent requests might be very easy to process and take no time at all – but that won’t matter if the customer experiences a long wait (long response time) because of the previous slow requests. Therefore, **it’s crucial to measure the response time on the client side rather than the server side.**
These are just a few things to take into account when addressing scalability. The most important thing I took away from this discussion is how to look at the various load parameters, asking important questions, like which operational processes are using the most resources and how can we better allocate those resources to speed up computations, and not ignoring the easy-to-ignore problems, such as high tail latencies. There is no one architecture that fits all applications – you have to ask yourselves these important questions before beginning to think of elasticity vs manual scaling or horizontal vs vertical scaling. I’ll end this section of scalability with a paragraph I found particularly inciteful: “**An architecture that scales well for a particular application is built around assumptions of which operations will be common and which will be rare—the load parameters. If those assumptions turn out to be wrong, the engineering effort for scaling is at best wasted, and at worst counterproductive.**”

## Maintainability 
Maintainability is all about making life easier for the engineering teams and operations teams that will be working on the system well into the future. This encompasses three key concepts: operability, simplicity, and evolvability. Operability means making it easy to things that need to be done often. Things that help achieve good operability are things like easy-to-understand, well-written documentation, avoiding machine dependency, and providing visibility into processes and runtime operations, which make it easy to monitor how the system is performing. Simplicity is pretty self-explanatory – remove unnecessary complexity from the system. Complexity is inevitable in large projects. As the code base grows, it will intertwine with itself, depend on certain modules, and generally be very messy take a long time to follow from one end to the other. Abstractions are a nice way to hide or remove complexity, but building abstractions can be difficult. It’s also important to not abstract too much freedom away, as that will make it difficult to do niche, specific tasks. Finally, evolvability is about making it easy to change the system without breaking it. There is virtually zero chance that business needs will remain static forever. Change can’t be avoided. There are processes that can help increase the evolvability of a system, such as TDD, but the real driver of evolvability will be how well the other pillars of maintainability are implemented: the operability and simplicity of the system. Obviously, simple and easy-to-operate systems are much easier to extend. A maintainable system needs all three core concepts to be realized. 
## Conclusion
This has been a very promising start to a highly anticipated book on my to-read list. Although this first chapter was fairly basic, I already learned a lot, and I’m very much looking forward to continuing my learning journey with *Designing Data-Intensive Applications.*
